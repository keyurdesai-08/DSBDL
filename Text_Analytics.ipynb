{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q8Gy7Wb5bHfw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "from nltk import pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15N2om8BcqlZ",
        "outputId": "ce4acbc7-130b-4897-d0bc-5a69cc412b7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WxbGaEJczPM",
        "outputId": "15a05640-8334-4448-8962-20e2d01ffc37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"Text analytics is the process of deriving insights from text data\""
      ],
      "metadata": {
        "id": "o8e3ZvdmdYn7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(doc)"
      ],
      "metadata": {
        "id": "Tvm27U6EdNOk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]"
      ],
      "metadata": {
        "id": "w3-mU3UIdOG_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n"
      ],
      "metadata": {
        "id": "LUxoSIIodXO2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n"
      ],
      "metadata": {
        "id": "2Y-vbmNjdnkg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokens = pos_tag(filtered_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "H0tCNsbGdvnc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Text:\", doc)\n",
        "print(\"Tokenized Text:\", tokens)\n",
        "print(\"Filtered Text (Stop Words Removed):\", filtered_tokens)\n",
        "print(\"Stemmed Text:\", stemmed_tokens)\n",
        "print(\"Lemmatized Text:\", lemmatized_tokens)\n",
        "print(\"POS Tagged Text:\", pos_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2j2ydfZd69b",
        "outputId": "6e78c860-a8d0-4a0f-d14c-1ad90ee75626"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Text analytics is the process of deriving insights from text data\n",
            "Tokenized Text: ['Text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'insights', 'from', 'text', 'data']\n",
            "Filtered Text (Stop Words Removed): ['Text', 'analytics', 'process', 'deriving', 'insights', 'text', 'data']\n",
            "Stemmed Text: ['text', 'analyt', 'process', 'deriv', 'insight', 'text', 'data']\n",
            "Lemmatized Text: ['Text', 'analytics', 'process', 'deriving', 'insight', 'text', 'data']\n",
            "POS Tagged Text: [('Text', 'JJ'), ('analytics', 'NNS'), ('process', 'NN'), ('deriving', 'VBG'), ('insights', 'NNS'), ('text', 'NN'), ('data', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPIFSauCd-q8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}